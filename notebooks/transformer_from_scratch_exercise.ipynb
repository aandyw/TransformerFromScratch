{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Transformer From Scratch\n",
                "\n",
                "## Table of Contents\n",
                "- [Introduction](#introduction)\n",
                "- [Setup](#setup)\n",
                "- [Embeddings](#embeddings)\n",
                "  - [Input Embeddings](#input-embeddings)\n",
                "  - [Positional Embeddings](#positional-embeddings)\n",
                "- [Attention Layers](#attention-layers)\n",
                "  - [Scaled Dot Product Attention](#scaled-dot-product-attention)\n",
                "  - [Multi-Headed Attention](#multi-headed-attention)\n",
                "- [Feed-Forward Network](#feed-forward-network)\n",
                "- [Intermediate Layers](#intermediate-layers)\n",
                "  - [Layer Normalization](#layer-normalization)\n",
                "  - [Residual Connections](#residual-connections)\n",
                "  - [Linear Layer](#linear-layer)\n",
                "- [Encoder-Decoder Structure](#encoder-decoder-structure)\n",
                "  - [Encoder](#encoder)\n",
                "  - [Decoder](#decoder)\n",
                "- [Transformer](#transformer)\n",
                "\n",
                "## Introduction\n",
                "The goal of this notebook is to provide a practical resource to systematically review and learn the underlying Transformer model architecture from the \"[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\" paper.\n",
                "\n",
                "<img src=\"images/transformer.png\" width=\"400\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch import Tensor"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Embeddings\n",
                "\n",
                "<img src=\"images/embeddings.png\" width=\"600\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Input Embeddings\n",
                "The input tokens passed through the Transformer model are first convered to vectors of dimension $d_{model}$ through a learned embedding, which we call the **Input Embeddings**.\n",
                "\n",
                "In section **3.4 Embeddings and Softmax** of the paper, the authors state that the output of the embedding layer is multiplied by $\\sqrt{d_{model}}$.\n",
                "\n",
                "Another thing to note is that input tokens are of type `int64`. These integer token indicies are then mapped to dense vectors via the learned embedding resulting in continuous floating-point vectors of type `float32`. All computations later (e.g. self-attention layers, feedforward networks, etc.) are done using floating-point operations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class InputEmbeddings(nn.Module):\n",
                "    def __init__(self, d_model: int, vocab_size: int):\n",
                "        \"\"\"\n",
                "        Embedding layer for input tokens\n",
                "\n",
                "        Args:\n",
                "            d_model (int): Hidden dimension of the model. The size of the vector\n",
                "                representations (embeddings / hidden states) used throughout the\n",
                "                Transformer model.\n",
                "            vocab_size (int): Size of the vocabulary. Number of unique tokens in the\n",
                "                input data.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.vocab_size = vocab_size\n",
                "\n",
                "        # Input to embedding layer: (*)\n",
                "        # Output from embedding layer: (*, H), where H is the hidden dim of the model.\n",
                "\n",
                "        # TODO: Create an embedding layer of size (vocab_size, d_model)\n",
                "        self.embedding = ...\n",
                "\n",
                "    def forward(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Embed input tokens.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): Input tokens of shape `(bs, seq_len)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: Embedded input of shape `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "        # seq_len dimension contains token ids that can be mapped back to unique word\n",
                "\n",
                "        ### REFER TO 3.4 Embeddings and Softmax in paper ###\n",
                "        # TODO: Return the result of the embedded `x` tensor\n",
                "        return ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Positional Embeddings\n",
                "The parallel nature of Transformers means that it lacks positional information about token order compared to sequential models like Recurrent Neuralnets (RNNs). We can resolve this by applying **Positional Encoding**.\n",
                "\n",
                "The paper uses sine and cosine functions for even and odd positions:\n",
                "$$\n",
                "\\text{PE}_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) \n",
                "$$\n",
                "\n",
                "$$\n",
                "\\text{PE}_{(pos, 2i + 1)} = \\cos(pos / 10000^{2i/d_{model}})\n",
                "$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class PositionalEncoding(nn.Module):\n",
                "    pe: Tensor\n",
                "\n",
                "    def __init__(self, d_model: int, max_seq_len: int, dropout: float = 0.1):\n",
                "        \"\"\"\n",
                "        Positional encoding / embeddings for input tokens\n",
                "\n",
                "        Args:\n",
                "            d_model (int): Hidden dimension of the model. The size of the vector\n",
                "                representations (embeddings / hidden states) used throughout the\n",
                "                Transformer model.\n",
                "            max_seq_len (int, optional): Maximum sequence length.\n",
                "            dropout (float, optional): Dropout rate. Defaults to 0.1.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.max_seq_len = max_seq_len\n",
                "\n",
                "        # TODO: create dropout\n",
                "        self.dropout = ...\n",
                "\n",
                "        # Create positional encodings of shape (max_seq_len, d_model)\n",
                "        pe = torch.zeros(max_seq_len, d_model)\n",
                "\n",
                "        ### REFER TO 3.5 Positional Encoding ###\n",
                "\n",
                "        # TODO: Create tensor of shape (max_seq_len, 1) with type `torch.float`\n",
                "        # Result: [[0, 1, ..., max_seq_len]]\n",
                "        pos = ...\n",
                "\n",
                "        # PE division term => 10000^-(2 * i / d_model)\n",
                "        div_term = torch.exp(\n",
                "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
                "        )\n",
                "\n",
                "        ### TODO: Use sine and cosine functions for even and odd positions\n",
                "        # NOTE: the above `div_term` is actually the reciprocal of the 10000^(2*i / d_model)\n",
                "\n",
                "        ...\n",
                "        \n",
                "        ###\n",
                "\n",
                "        # TODO: Add batch dimension to positional encodings\n",
                "        pe = ...  # (1, max_seq_len, d_model)\n",
                "\n",
                "        # Tensor is saved in file when model is saved.\n",
                "        self.register_buffer(\"pe\", pe) # Allows you to use `self.pe`\n",
                "\n",
                "    def forward(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Apply positional encoding to input embeddings.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): Input embeddings of shape `(bs, seq_len, d_model)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: Positional encodings of shape `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # Add positional encodings to input embeddings\n",
                "\n",
                "        # TODO: get the seq_len from `x`\n",
                "        seq_len = ...\n",
                "\n",
                "        # TODO: Shorten positional encodings if seq_len is greater than max_seq_len\n",
                "        pe_out = ...\n",
                "\n",
                "        # TODO: Add the positional information onto the input tensor `x`\n",
                "        x = ...\n",
                "\n",
                "        # TODO: Apply dropout\n",
                "        return ...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Attention Layers\n",
                "\n",
                "The **Multi-Head Attention block** is where the attention mechanism exists. It is computed fundamentally with scaled dot product attention.\n",
                "\n",
                "There are 2 types of attention in the Transformer: **Self-Attention** and **Cross-Attention**. They both use the same multi-head attention mechanism.\n",
                "\n",
                "The primary differences are:\n",
                "\n",
                "- **Self-Attention:** Queries, keys, and values come from the same input sequence.\n",
                "\n",
                "- **Cross Attention:** Queries come from one source (e.g., the decoder’s hidden state in a transformer), while keys and values come from another source (e.g., the encoder’s outputs).\n",
                "\n",
                "<br>\n",
                "\n",
                "<img src=\"images/attention.png\" width=\"600\">"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Scaled Dot Product Attention\n",
                "\n",
                "$$\n",
                "\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
                "$$\n",
                "\n",
                "The paper uses $d_{\\text{model}} = 512$ with $h=8$ parallel attention layers (heads).\n",
                "\n",
                "Therefore, the dimension of queries, keys, and values will be $d_k=d_v=d_{model}/h = 64$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@staticmethod\n",
                "def scaled_dot_product_attention(\n",
                "    q: Tensor, k: Tensor, v: Tensor, mask: Tensor | None, dropout: nn.Dropout | None\n",
                ") -> tuple[Tensor, Tensor]:\n",
                "    \"\"\"Compute Scaled Dot Product Attention.\"\"\"\n",
                "\n",
                "    d_k = q.shape[-1]\n",
                "\n",
                "    # TODO: Compute attention scores (not applying softmax) with\n",
                "    # @ operation (matrix multiply) and .transpose()\n",
                "\n",
                "    # (bs, num_heads, seq_len, d_k) -> (bs, num_heads, seq_len, seq_len)\n",
                "    scores = ...\n",
                "\n",
                "    if mask is not None:\n",
                "        # For all values in mask == 0 replace with -inf\n",
                "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
                "\n",
                "    # TODO: Apply softmax to last dim\n",
                "    # Each row is a query, each column is a key. You want to convert raw scores over keys\n",
                "    # into a probability distribution. In other words, you want each row / query to have\n",
                "    # weights that sum to 1.\n",
                "    scores = ...  # (bs, num_heads, seq_len, seq_len)\n",
                "\n",
                "    if dropout is not None:\n",
                "        scores = dropout(scores)\n",
                "\n",
                "    # TODO: Multiply by values\n",
                "    weights = ...  # (bs, num_heads, seq_len, d_k)\n",
                "\n",
                "    # We return the scores for visualization\n",
                "    return weights, scores"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multi-Headed Attention\n",
                "\n",
                "Each head of attention is computed with the above scaled dot-product attention and then concatenated.\n",
                "\n",
                "$$\n",
                "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\cdots, \\text{head}_h) W^O\n",
                "$$\n",
                "$$\n",
                "\\text{where head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
                "$$\n",
                "\n",
                "The projection matrix $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$ ensures that the concatenation of the heads $h \\cdot d_v$ is projected back into $d_{model}$ which is the desired output dimension of the multi-headed attention.\n",
                "\n",
                "However, in the original paper, $h \\cdot d_v = d_{model}$ so we can ignore the details."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import math\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "from torch import Tensor\n",
                "\n",
                "\n",
                "class MultiHeadAttention(nn.Module):\n",
                "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
                "        super().__init__()\n",
                "        self.d_model = d_model\n",
                "        self.num_heads = num_heads  # aka. `h`\n",
                "\n",
                "        # TODO: Get the dimension of d_k, d_v\n",
                "        self.d_k = ...  # d_k = d_v\n",
                "\n",
                "        self.W_q = nn.Linear(d_model, d_model)\n",
                "        self.W_k = nn.Linear(d_model, d_model)\n",
                "        self.W_v = nn.Linear(d_model, d_model)\n",
                "        self.W_o = nn.Linear(d_model, d_model)\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "\n",
                "    def forward(self, q: Tensor, k: Tensor, v: Tensor, mask: Tensor | None) -> Tensor:\n",
                "        \"\"\"Compute Multi-Headed Attention.\"\"\"\n",
                "\n",
                "\n",
                "        # TODO: Create `query`, `key`, `value` tensors\n",
                "        query = ...  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
                "        key = ...  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
                "        value = ...  # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
                "\n",
                "        # TODO: Split into multiple heads with .view()\n",
                "        # (bs, seq_len, d_model) -> (bs, seq_len, num_heads, d_k)\n",
                "        query = ...\n",
                "\n",
                "        # TODO: Use .transpose()\n",
                "        # (bs, seq_len, num_heads, d_k) -> (bs, num_heads, seq_len, d_k)\n",
                "        query = ...\n",
                "\n",
                "        key = ...\n",
                "        key = ...\n",
                "\n",
                "        value = ...\n",
                "        value = ...\n",
                "\n",
                "        weights, scores = scaled_dot_product_attention(\n",
                "            query, key, value, mask, self.dropout\n",
                "        )\n",
                "\n",
                "        ### Perform concatenation of the heads ###\n",
                "\n",
                "        # TODO: Use .transpose()\n",
                "        # (bs, num_heads, seq_len, d_k) -> (bs, seq_len, num_heads, d_k)\n",
                "        weights = ...\n",
                "\n",
                "        weights = weights.contiguous()\n",
                "\n",
                "        # TODO: Use .view() to concatenate the heads\n",
                "        # (bs, seq_len, num_heads, d_k) -> (bs, seq_len, d_model)\n",
                "        concat = ...\n",
                "\n",
                "        # TODO: Apply W_o projection\n",
                "        # (bs, seq_len, d_model) -> (bs, seq_len, d_model)\n",
                "        return ...\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feed-Forward Network\n",
                "The Feed-Forward Network (FFN) in a Transformer is applied independently to each token position after the Multi-Head Self-Attention Mechanism. It consists of two linear layers with a non-linearity:\n",
                "\n",
                "$$\n",
                "\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2\n",
                "$$\n",
                "\n",
                "It can also be expressed with the $\\text{ReLU}$ activation function which squashes negative inputs to $0$.\n",
                "\n",
                "$$\n",
                "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
                "$$\n",
                "\n",
                "This will be helpful:\n",
                "> The dimensionality of input and output is $d_{model} = 512$, and the inner-layer has dimensionality $d_{ff} = 2048$."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class FeedForwardBlock(nn.Module):\n",
                "    def __init__(self, d_model: int = 512, d_ff: int = 2048, dropout: float = 0.1):\n",
                "        super().__init__()\n",
                "\n",
                "        ### REFER TO Section 3.3 Position-wise Feed-Forward Networks ###\n",
                "\n",
                "        # TODO: Create the two linear transformations between\n",
                "        self.linear1 = ...\n",
                "        self.dropout = ...\n",
                "        self.linear2 = ...\n",
                "\n",
                "    def forward(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        1. (batch_size, seq_len, d_model) -> (batch_size, seq_len, d_ff)\n",
                "        2. (batch_size, seq_len, d_ff) -> (batch_size, seq_len, d_model)\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): The input tensor. `(batch_size, seq_len, d_model)`\n",
                "        Returns:\n",
                "            Tensor: The output tensor. `(batch_size, seq_len, d_model)`\n",
                "        \"\"\"\n",
                "        \n",
                "        ### TODO: Create forward pass. Apply dropout after ReLU.\n",
                "        # Use torch.relu()\n",
                "\n",
                "        x = ...\n",
                "\n",
                "        ###\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Intermediate Layers"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Layer Normalization"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "LayerNorm operates independently on each sample within a batch, unlike BatchNorm, which normalizes across the batch dimension. It normalizes the inputs across the feature dimension.\n",
                "\n",
                "**Purpose:** Mitigate internal covariate shift thus improving training speed, stability, and convergence of the model. Also, improves generalization."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LayerNormalization(nn.Module):\n",
                "    def __init__(self, eps: float = 1e-6):\n",
                "        \"\"\"\n",
                "        Args:\n",
                "            eps (float, optional): Epsilon value to avoid division by zero.\n",
                "                Defaults to 1e-6.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.eps = eps\n",
                "\n",
                "        # Two learnable parameters\n",
                "        self.alpha = nn.Parameter(torch.ones(1))  # Scale parameter (Multiplicative)\n",
                "        self.bias = nn.Parameter(torch.zeros(1))  # Shift parameter (Additive)\n",
                "\n",
                "    def forward(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Apply layer norm to last dimension of the input tensor.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): `(bs, seq_len, d_model)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Apply mean & std to last dimension\n",
                "        mean = ... # (bs, seq_len, 1)\n",
                "        std = ...  # (bs, seq_len, 1)\n",
                "\n",
                "        std = std + self.eps\n",
                "\n",
                "        # TODO: normalize x\n",
                "        x = ...\n",
                "\n",
                "        # TODO: scale by alpha\n",
                "        x = ...\n",
                "\n",
                "        # TODO: add bias\n",
                "        x = ...\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Residual Connections\n",
                "\n",
                "The paper defines the residual connection implementation as\n",
                "$$\n",
                "\\text{LayerNorm}(x + \\text{Sublayer}(x))\n",
                "$$\n",
                "\n",
                "However, we will follow [The Annotated Transformer's](https://nlp.seas.harvard.edu/2018/04/03/attention.html) implementation by applying dropout to the output of each normalized sub-layer, before adding it to the input.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ResidualConnection(nn.Module):\n",
                "    def __init__(self, dropout: float = 0.1):\n",
                "        super().__init__()\n",
                "        self.dropout = nn.Dropout(dropout)\n",
                "        self.norm = LayerNormalization()\n",
                "\n",
                "    def forward(self, x: Tensor, sublayer: nn.Module) -> Tensor:\n",
                "        \"\"\"\n",
                "        Residual connection with layer normalization.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): `(bs, seq_len, d_model)`.\n",
                "            sublayer (nn.Module): The intermediate layer to wrap w/ residual connection.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Apply dropout to sublayer before \n",
                "        return ..."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Linear Layer\n",
                "\n",
                "This layer is a projection from $d_{model}$ into log probabilities across the entire vocab."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LinearLayer(nn.Module):\n",
                "    def __init__(self, d_model: int, vocab_size: int):\n",
                "        \"\"\"\n",
                "        Linear Layer is a projection layer that converts the embedding into the\n",
                "        vocabulary.\n",
                "\n",
                "        Args:\n",
                "            d_model (int): The size of the model's hidden dimension.\n",
                "            vocab_size (int): The size of the vocabulary.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "\n",
                "        # TODO: Create a linear layer of size (d_model, vocab_size)\n",
                "        self.linear = ...\n",
                "\n",
                "    def forward(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Apply projection on embeddings.\n",
                "        Output will be a log probability distribution over the vocabulary.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): `(bs, seq_len, d_model)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, vocab_size)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # (bs, seq_len, d_model) -> (bs, seq_len, vocab_size)\n",
                "        # TODO: Apply projection\n",
                "        out = ...\n",
                "\n",
                "        # TODO: Return log probabilities not probabilities to vocab_size\n",
                "        return torch.log_softmax(..., dim=...)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Encoder-Decoder Structure\n",
                "We can finally put everything together\n",
                "\n",
                "<img src=\"images/transformer.png\" width=\"400\">"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# dummy variables\n",
                "src_vocab_size = 1000\n",
                "tgt_vocab_size = 1000\n",
                "src_seq_len = 100\n",
                "tgt_seq_len = 100\n",
                "\n",
                "d_model = 512  # hidden dimension of the model\n",
                "num_blocks = 6  # number of encoder and decoder blocks\n",
                "num_heads = 8  # number of attention heads\n",
                "d_ff = 2048  # size of the feed-forward layer\n",
                "dropout = 0.1  # dropout rate"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Encoder"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class EncoderBlock(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        self_attention_block: MultiHeadAttention,\n",
                "        feed_forward_block: FeedForwardBlock,\n",
                "        dropout: float = 0.1,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.self_attention_block = self_attention_block\n",
                "        self.feed_forward_block = feed_forward_block\n",
                "        self.residual_connections = nn.ModuleList(\n",
                "            [ResidualConnection(dropout) for _ in range(2)]\n",
                "        )\n",
                "\n",
                "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the encoder block.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): `(bs, seq_len, d_model)`.\n",
                "            src_mask (Tensor): The mask for the source language `(bs, 1, 1, seq_len)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        ### TODO: Create the encoder block forward pass ###\n",
                "        # NOTE: the second param of the residual connection requires a layer\n",
                "\n",
                "        x = self.residual_connections[0](\n",
                "            x, lambda x: ...\n",
                "        ) # Residual connection on self attention block\n",
                "\n",
                "        # Residual connection on feed forward block\n",
                "        x = self.residual_connections[1](x, ...)\n",
                "        ###\n",
                "\n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder_blocks: list[EncoderBlock] = []\n",
                "\n",
                "for _ in range(num_blocks):\n",
                "    # TODO: Create and append Encoder Blocks here\n",
                "    ...\n",
                "\n",
                "encoder_layers = nn.ModuleList(encoder_blocks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Encoder(nn.Module):\n",
                "    def __init__(self, layers: nn.ModuleList):\n",
                "        super().__init__()\n",
                "        self.layers = layers # the encoder blocks\n",
                "        self.norm = LayerNormalization()\n",
                "\n",
                "    def forward(self, x: Tensor, src_mask: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Foward pass through the encoder.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): The input to the encoder.\n",
                "            src_mask (Tensor): The mask for the source language.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: A tensor of `(batch_size, seq_len, d_model)` represents a sequence\n",
                "                of context-rich embeddings that encode the input sequence's semantic and\n",
                "                positional information.\n",
                "        \"\"\"\n",
                "        for layer in self.layers:\n",
                "            x = layer(x, src_mask)\n",
                "\n",
                "        # Apply a final layer normalization after all encoder blocks\n",
                "        return self.norm(x)\n",
                "    \n",
                "encoder = Encoder(encoder_layers)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Decoder\n",
                "\n",
                "To reiterate, for **Cross Attention**, queries come from one source (e.g., the decoder’s hidden state in a transformer), while keys and values come from another source (e.g., the encoder’s outputs)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class DecoderBlock(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        self_attention_block: MultiHeadAttention,\n",
                "        cross_attention_block: MultiHeadAttention,\n",
                "        feed_forward_block: FeedForwardBlock,\n",
                "        dropout: float = 0.1,\n",
                "    ):\n",
                "        \"\"\"\n",
                "        Decoder block contains:\n",
                "            1. (Masked Multi-Head Attention) A self-attention block where `qkv` come\n",
                "                from decoder's input embedding.\n",
                "            2. (Multi-Head Attention) A cross-attention block where `q` come from\n",
                "                decoder and `k`,`v` come from encoder outputs.\n",
                "        \"\"\"\n",
                "        super().__init__()\n",
                "        self.self_attention_block = self_attention_block\n",
                "        self.cross_attention_block = cross_attention_block\n",
                "        self.feed_forward_block = feed_forward_block\n",
                "        self.residual_connections = nn.ModuleList(\n",
                "            [ResidualConnection(dropout) for _ in range(3)]\n",
                "        )\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        x: Tensor,\n",
                "        encoder_output: Tensor,\n",
                "        src_mask: Tensor,\n",
                "        tgt_mask: Tensor,\n",
                "    ) -> Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the decoder block.\n",
                "        Decoder block ussed for machine-translation to go from source to target lang.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): The decoder input `(bs, seq_len, d_model)`.\n",
                "            encoder_output (Tensor): `(bs, seq_len, d_model)`.\n",
                "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
                "            tgt_mask (Tensor): `(bs, 1, seq_len, seq_len)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "        \n",
                "        ### TODO: Create the decoder block forward pass ###\n",
                "\n",
                "        # Residual connection on self attention block\n",
                "        x = self.residual_connections[0](\n",
                "            x, lambda x: ...\n",
                "        )\n",
                "\n",
                "        # Residual connection around cross-attention block\n",
                "        # Use encoder output for the keys and values\n",
                "        x = self.residual_connections[1](\n",
                "            x,\n",
                "            lambda x: ...\n",
                "        )\n",
                "\n",
                "        # Residual connection on feed forward block\n",
                "        x = self.residual_connections[2](x, ...)\n",
                "        ###\n",
                "        \n",
                "        return x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "decoder_blocks: list[DecoderBlock] = []\n",
                "\n",
                "for _ in range(num_blocks):\n",
                "    # TODO: Create and append Decoder Blocks here\n",
                "    ...\n",
                "\n",
                "decoder_layers = nn.ModuleList(decoder_blocks)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Decoder(nn.Module):\n",
                "    def __init__(self, layers: nn.ModuleList):\n",
                "        super().__init__()\n",
                "        self.layers = layers # decoder blocks\n",
                "        self.norm = LayerNormalization()\n",
                "\n",
                "    def forward(\n",
                "        self,\n",
                "        x: Tensor,\n",
                "        encoder_output: Tensor,\n",
                "        src_mask: Tensor,\n",
                "        tgt_mask: Tensor,\n",
                "    ) -> Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the decoder.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): The input to the decoder block.\n",
                "            encoder_output (Tensor): The output from the encoder.\n",
                "            src_mask (Tensor): The mask used for the source language (e.g. English).\n",
                "            tgt_mask (Tensor): The mask used for the target language (e.g. German).\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "        for layer in self.layers:\n",
                "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
                "\n",
                "        return self.norm(x)\n",
                "\n",
                "decoder = Decoder(decoder_layers)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Transformer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class Transformer(nn.Module):\n",
                "    def __init__(\n",
                "        self,\n",
                "        encoder: Encoder,\n",
                "        decoder: Decoder,\n",
                "        src_embed: InputEmbeddings,\n",
                "        tgt_embed: InputEmbeddings,\n",
                "        src_pos: PositionalEncoding,\n",
                "        tgt_pos: PositionalEncoding,\n",
                "        projection_layer: LinearLayer,\n",
                "    ):\n",
                "        super().__init__()\n",
                "        self.encoder = encoder\n",
                "        self.decoder = decoder\n",
                "        self.src_embed = src_embed\n",
                "        self.tgt_embed = tgt_embed\n",
                "        self.src_pos = src_pos\n",
                "        self.tgt_pos = tgt_pos\n",
                "        self.projection_layer = projection_layer\n",
                "\n",
                "    def encode(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
                "        \"\"\"Forward pass through the encoder with input tokens of type int64.\n",
                "\n",
                "        Args:\n",
                "            src (Tensor): `(bs, seq_len)`.\n",
                "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # Embedding maps token ids to dense vectors of type float32\n",
                "\n",
                "        # TODO: Embed source tokens\n",
                "        src = ...  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
                "        \n",
                "        # TODO: Apply positional encoding\n",
                "        src = ...\n",
                "\n",
                "        return ...\n",
                "\n",
                "    def decode(\n",
                "        self, encoder_output: Tensor, src_mask: Tensor, tgt: Tensor, tgt_mask: Tensor\n",
                "    ) -> Tensor:\n",
                "        \"\"\"\n",
                "        Forward pass through the decoder.\n",
                "        - Encoder output is used in the cross-attention block and is of type float32.\n",
                "        - Target tokens are still of type int64 and need to be embedded with input\n",
                "        embeddings + positional encoding.\n",
                "\n",
                "        Args:\n",
                "            encoder_output (Tensor): `(bs, seq_len, d_model)`.\n",
                "            src_mask (Tensor): `(bs, 1, 1, seq_len)`.\n",
                "            tgt (Tensor): `(bs, seq_len)`.\n",
                "            tgt_mask (Tensor): `(bs, 1, seq_len, seq_len)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, d_model)`.\n",
                "        \"\"\"\n",
                "\n",
                "        # TODO: Embed target tokens\n",
                "        tgt = ...  # (bs, seq_len) -> (bs, seq_len, d_model)\n",
                "        \n",
                "        # TODO: Apply positional encoding\n",
                "        tgt = ...\n",
                "\n",
                "        # TODO: Forward pass through the decoder\n",
                "        return ...\n",
                "\n",
                "    def project(self, x: Tensor) -> Tensor:\n",
                "        \"\"\"\n",
                "        Project the output of the decoder to the target vocabulary size.\n",
                "\n",
                "        Args:\n",
                "            x (Tensor): The output of the decoder `(bs, seq_len, d_model)`.\n",
                "\n",
                "        Returns:\n",
                "            Tensor: `(bs, seq_len, vocab_size)`.\n",
                "        \"\"\"\n",
                "\n",
                "        return self.projection_layer(x)\n",
                "\n",
                "# TODO: Create embedding layers\n",
                "src_embed = ...\n",
                "tgt_embed = ...\n",
                "\n",
                "# TODO: Create positional encoding layers\n",
                "src_pos = ...\n",
                "tgt_pos = ...\n",
                "\n",
                "# TODO: Create projection layer\n",
                "projection_layer = ...\n",
                "\n",
                "transformer = Transformer(\n",
                "    encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
